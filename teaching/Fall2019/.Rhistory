names(iris)
table(iris$Species)
summary(iris$Species)
w <- iris[[2]] #Column Sepal.Width
mean(w)
# equivalent
w = iris$Sepal.Width
w = iris[,2] # more general, can select multiple elements
library(ggplot2)
ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width,
color = Species, shape = Species)) + geom_point(size = 2)
ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot()
ggplot(iris, aes(Species, Sepal.Length)) + geom_violin()
ggplot(iris, aes(Species, Sepal.Length)) +
geom_boxplot() + coord_flip()
ggplot(iris, aes(Species, Sepal.Length)) +
geom_violin() + coord_flip()
?geom_violin
dev.off()
rm(list = ls())
# Calculator
pi
exp(1)
log(2)
sqrt(pi)
#density of standard normal at value 2
1/sqrt(2*pi) * exp(-2)
dnorm(2)
?dnorm
# Basic graphics
plot(cars, xlab="Speed", ylab="Distance to Stop",
main="Stopping Distance for Cars in 1920")
cars
# Powerful seq function
seq(0, 3, 0.5)
?t
t
class(t)
g
x = 1:6
x
6:1
sample(x) #permutation of all elements of x
k=3
sample(x, size=k) #permutation of k elements of x
set.seed(123)
sample(x)
sample(x, size=k)
set.seed(123)
sample(x)
sample(x, size=k)
sample(x)
sample(x)
# rolls n fair dice and returns the sum
sumdice <- function(n) {
k <- sample(1:6, size=n, replace=TRUE)
return(sum(k))
}
sumdice(2)
a <- sumdice(10000)
a / 10000
help("iris")
class(iris)
class(iris)
dim(iris)
head(iris)
head(iris)
names(iris)
head(iris)
names(iris)
table(iris$Species)
summary(iris$Species)
library(ggplot2)
library(ggplot2)
ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width,
color = Species, shape = Species)) + geom_point(size = 2)
ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot()
ggplot(iris, aes(Species, Sepal.Length)) + geom_violin()
ggplot(iris, aes(Species, Sepal.Length)) +
geom_boxplot() + coord_flip()
ggplot(iris, aes(Species, Sepal.Length)) +
geom_violin() + coord_flip()
#########################
# Arrays
# An array is a multiply subscripted collection of a single type of data.
x <- 1:24 # vector
dim(x) <- length(x) # 1 dimensional array
matrix(1:24, nrow=4, ncol=6) # 4 by 6 matrix
n <- 1000
k <- 0      #counter for accepted
j <- 0      #iterations
y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1)  #random variate from g
if (x * (1-x) > u) {
#we accept x
k <- k + 1
y[k] <- x
}
}
j
# change c to be 1.5
n <- 1000
k <- 0      #counter for accepted
j <- 0      #iterations
y <- numeric(n)
while (k < n) {
u <- runif(1)
j <- j + 1
x <- runif(1)  #random variate from g
if (4*x * (1-x) > u) {
#we accept x
k <- k + 1
y[k] <- x
}
}
j
#compare empirical and theoretical percentiles
p <- seq(.1, .9, .1)
Qhat <- quantile(y, p)   #quantiles of sample
Q <- qbeta(p, 2, 2)      #theoretical quantiles
# se <- sqrt(p * (1-p) / (n * dbeta(Q, 2, 2)^2)) #see Ch. 2
round(rbind(Qhat, Q), 3)
# se <- sqrt(p * (1-p) / (n * dbeta(Q, 2, 2)^2)) #see Ch. 2
round(rbind(Qhat, Q), 3)
n <- 1000
u <- runif(n)
x <- u^(1/3)
hist(x, prob = TRUE, main = expression(f(x)==3*x^2)) #density histogram of sample
y <- seq(0, 1, .01)
lines(y, 3*y^2)    #density curve f(x)
n <- 100000
p <- 0.4
u <- runif(n)
x <- as.integer(u > 0.6)   #(u > 0.6) is a logical vector
mean(x) # p=0.4
var(x)  # p*(1-p)=0.24
### Example 5.1 (Simple Monte Carlo integration)
m <- 10000
x <- runif(m)
theta.hat <- mean(exp(-x))
print(theta.hat)
print(1 - exp(-1))
### Example 5.2 (Simple Monte Carlo integration, cont.)
m <- 10000
x <- runif(m, min=2, max=4)
theta.hat <- mean(exp(-x)) * 2
print(theta.hat)
print(exp(-2) - exp(-4))
### Example 5.3 (Monte Carlo integration, unbounded interval)
x <- seq(.1, 2.5, length = 10)
m <- 10000
u <- runif(m)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i] * exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
Phi <- pnorm(x)
print(round(rbind(x, cdf, Phi), 3))
### Example 5.4 (Example 5.3, cont.)
x <- seq(.1, 2.5, length = 10)
m <- 10000
z <- rnorm(m)
dim(x) <- length(x)
p <- apply(x, MARGIN = 1,
FUN = function(x, z) {mean(z < x)}, z = z)
Phi <- pnorm(x)
print(round(rbind(x, p, Phi), 3))
qqplot(z1^2+z2^2, -2*log(u), cex=0.25, xlab="R2", ylab="-2logU", main="Box-Muller verification")
### Verification of the Box-Muller
n=1000
z1=rnorm(n); z2=rnorm(n)
u=runif(n)
qqplot(z1^2+z2^2, -2*log(u), cex=0.25, xlab="R2", ylab="-2logU", main="Box-Muller verification")
abline(0, 1)
n <- 1000
a <- 3
b <- 2
u <- rgamma(n, shape=a, rate=1)
v <- rgamma(n, shape=b, rate=1)
x <- u / (u + v)
q <- qbeta(ppoints(n), a, b)
qqplot(q, x, cex=0.25, xlab="Beta(3, 2)", ylab="Sample")
abline(0, 1)
n <- 10000
x1 <- rnorm(n, 0, 1)
x2 <- rnorm(n, 10, 1)
s <- x1 + x2              #the convolution
u <- runif(n)
k <- as.integer(u < 0.25)  #vector of 0's and 1's
x <- k * x1 + (1-k) * x2  #the mixture
par(mfcol=c(1,2))         #two graphs per page
hist(x, prob=TRUE, breaks = 30, main = "Histogram of the mixture")
hist(s, prob=TRUE, breaks = 30, main = "Histogram of the sum")
n <- 10000
nu <- 2
X <- matrix(rnorm(n*nu), n, nu)^2 #matrix of sq. normals
#sum the squared normals across each row:
#method 1
y <- rowSums(X)
#method 2
y <- apply(X, MARGIN=1, FUN=sum)  #a vector length n
mean(y)
var(y)
# mean and covariance parameters
mu <- c(0, 0)
Sigma <- matrix(c(1, .9, .9, 1), nrow = 2, ncol = 2)
rmvn.eigen <- function(n, mu, Sigma) {
# generate n random vectors from MVN(mu, Sigma)
# dimension is inferred from mu and Sigma
d <- length(mu)
ev <- eigen(Sigma, symmetric = TRUE)
lambda <- ev$values
V <- ev$vectors
R <- V %*% diag(sqrt(lambda)) %*% t(V)
Z <- matrix(rnorm(n*d), nrow = n, ncol = d)
X <- Z %*% R + matrix(mu, n, d, byrow = TRUE)
X
}
dev.off()
# mean and covariance parameters
mu <- c(0, 0)
Sigma <- matrix(c(1, .9, .9, 1), nrow = 2, ncol = 2)
rmvn.eigen <- function(n, mu, Sigma) {
# generate n random vectors from MVN(mu, Sigma)
# dimension is inferred from mu and Sigma
d <- length(mu)
ev <- eigen(Sigma, symmetric = TRUE)
lambda <- ev$values
V <- ev$vectors
R <- V %*% diag(sqrt(lambda)) %*% t(V)
Z <- matrix(rnorm(n*d), nrow = n, ncol = d)
X <- Z %*% R + matrix(mu, n, d, byrow = TRUE)
X
}
# generate the sample
X <- rmvn.eigen(1000, mu, Sigma)
plot(X, xlab = "x", ylab = "y", pch = 20)
print(colMeans(X))
rmvn.svd <- function(n, mu, Sigma) {
# generate n random vectors from MVN(mu, Sigma)
# dimension is inferred from mu and Sigma
d <- length(mu)
S <- svd(Sigma)
R <- S$u %*% diag(sqrt(S$d)) %*% t(S$v) #sq. root Sigma
Z <- matrix(rnorm(n*d), nrow=n, ncol=d)
X <- Z %*% R + matrix(mu, n, d, byrow=TRUE)
X
}
rmvn.Choleski <-
function(n, mu, Sigma) {
# generate n random vectors from MVN(mu, Sigma)
# dimension is inferred from mu and Sigma
d <- length(mu)
Q <- chol(Sigma) # Choleski factorization of Sigma
Z <- matrix(rnorm(n*d), nrow=n, ncol=d)
X <- Z %*% Q + matrix(mu, n, d, byrow=TRUE)
X
}
library(MASS)
library(mvtnorm)
n <- 100          #sample size
d <- 30           #dimension
N <- 2000         #iterations
mu <- numeric(d)
set.seed(100)
st.eigen = system.time(for (i in 1:N)
rmvn.eigen(n, mu, cov(matrix(rnorm(n*d), n, d))))
set.seed(100)
st.svd = system.time(for (i in 1:N)
rmvn.svd(n, mu, cov(matrix(rnorm(n*d), n, d))))
set.seed(100)
st.chol = system.time(for (i in 1:N)
rmvn.Choleski(n, mu, cov(matrix(rnorm(n*d), n, d))))
set.seed(100)
st.mvrnorm = system.time(for (i in 1:N)
mvrnorm(n, mu, cov(matrix(rnorm(n*d), n, d))))
set.seed(100)
st.rmvnorm = system.time(for (i in 1:N)
rmvnorm(n, mu, cov(matrix(rnorm(n*d), n, d))))
timing = list(eigen = st.eigen, svd = st.svd, chol = st.chol,
mvrnorm = st.mvrnorm, rmvnorm = st.rmvnorm)
timing
### Example 5.1 (Simple Monte Carlo integration)
m <- 10000
x <- runif(m)
theta.hat <- mean(exp(-x))
print(theta.hat)
print(1 - exp(-1))
### Example 5.2 (Simple Monte Carlo integration, cont.)
m <- 10000
x <- runif(m, min=2, max=4)
theta.hat <- mean(exp(-x)) * 2
print(theta.hat)
print(exp(-2) - exp(-4))
### Example 5.3 (Monte Carlo integration, unbounded interval)
x <- seq(.1, 2.5, length = 10)
m <- 10000
u <- runif(m)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i] * exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
Phi <- pnorm(x)
print(round(rbind(x, cdf, Phi), 3))
### Example 5.4 (Example 5.3, cont.)
x <- seq(.1, 2.5, length = 10)
m <- 10000
z <- rnorm(m)
dim(x) <- length(x)
p <- apply(x, MARGIN = 1,
FUN = function(x, z) {mean(z < x)}, z = z)
Phi <- pnorm(x)
print(round(rbind(x, p, Phi), 3))
### Example 5.5 (Error bounds for MC integration)
x <- 2
m <- 10000
z <- rnorm(m)
g <- (z < x)  #the indicator function
v <- mean((g - mean(g))^2) / m
cdf <- mean(g)
c(cdf, v)
c(cdf - 1.96 * sqrt(v), cdf + 1.96 * sqrt(v))
my.cdf = pnorm(2)
my.v = my.cdf*(1-my.cdf)/m
c(my.cdf, my.v)
c(my.cdf - 1.96 * sqrt(my.v), my.cdf + 1.96 * sqrt(my.v))
MC.Phi <- function(x, R = 10000, antithetic = TRUE) {
u <- runif(R/2)
if (!antithetic) {v <- runif(R/2)} else{
v <- 1 - u
}
u <- c(u, v)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i] * exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
cdf
}
x <- seq(.1, 2.5, length=5)
Phi <- pnorm(x)
set.seed(123)
MC1 <- MC.Phi(x, anti = FALSE)
set.seed(123)
MC2 <- MC.Phi(x)
print(round(rbind(x, MC1, MC2, Phi), 5))
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1.95
for (i in 1:m) {
MC1[i] <- MC.Phi(x, R = 1000, anti = FALSE)
MC2[i] <- MC.Phi(x, R = 1000)
}
print(sd(MC1))
print(sd(MC2))
print((var(MC1) - var(MC2))/var(MC1))
m <- 10000
a <- - 12 + 6 * (exp(1) - 1)
U <- runif(m)
T1 <- exp(U)                  #simple MC
T2 <- exp(U) + a * (U - 1/2)  #controlled
c(mean(T1), sd(T1))
c(mean(T2), sd(T2))
(var(T1) - var(T2)) / var(T1)
f <- function(u)
exp(-.5)/(1+u^2)
g <- function(u)
exp(-u)/(1+u^2)
set.seed(510) #needed later
u <- runif(10000)
B <- f(u)
A <- g(u)
cor(A, B)
a <- -cov(A,B) / var(B)    #est of c*
a
m <- 100000
u <- runif(m)
T1 <- g(u)
T2 <- T1 + a * (f(u) - exp(-.5)*pi/4)
mean(a * (f(u) - exp(-.5)*pi/4))
c(mean(T1), mean(T2))
c(var(T1), var(T2))
(var(T1) - var(T2)) / var(T1)
set.seed(510)
u <- runif(10000)
f <- exp(-.5)/(1+u^2)
g <- exp(-u)/(1+u^2)
L <- lm(g ~ f)
summary(L)
c.star <-  - L$coeff[2]   # beta_1
mu <- exp(-.5)*pi/4
c.star
theta.hat <- sum(L$coeff * c(1, mu))  # pred. value at mu
predict(L, newdata = data.frame(f=mu)) # alternative
theta.hat
summary(L)$sigma
summary(L)$r.squared
### Example 5.1 (Simple Monte Carlo integration)
m <- 10000
x <- runif(m)
theta.hat <- mean(exp(-x))
print(theta.hat)
print(1 - exp(-1))
### Example 5.2 (Simple Monte Carlo integration, cont.)
m <- 10000
x <- runif(m, min=2, max=4)
theta.hat <- mean(exp(-x)) * 2
print(theta.hat)
print(exp(-2) - exp(-4))
### Example 5.3 (Monte Carlo integration, unbounded interval)
x <- seq(.1, 2.5, length = 10)
x
m <- 10000
u <- runif(m)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i] * exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
Phi <- pnorm(x)
print(round(rbind(x, cdf, Phi), 3))
print(round(rbind(x, cdf, Phi), 3))
print(round(rbind(x, cdf, Phi), 3))
### Example 5.4 (Example 5.3, cont.)
x <- seq(.1, 2.5, length = 10)
m <- 10000
z <- rnorm(m)
dim(x) <- length(x)
p <- apply(x, MARGIN = 1,
FUN = function(x, z) {mean(z < x)}, z = z)
Phi <- pnorm(x)
print(round(rbind(x, p, Phi), 3))
MC.Phi <- function(x, R = 10000, antithetic = TRUE) {
u <- runif(R/2)
if (!antithetic) {v <- runif(R/2)} else{
v <- 1 - u
}
u <- c(u, v)
cdf <- numeric(length(x))
for (i in 1:length(x)) {
g <- x[i] * exp(-(u * x[i])^2 / 2)
cdf[i] <- mean(g) / sqrt(2 * pi) + 0.5
}
cdf
}
x <- seq(.1, 2.5, length=5)
Phi <- pnorm(x)
set.seed(123)
MC1 <- MC.Phi(x, anti = FALSE)
set.seed(123)
MC2 <- MC.Phi(x)
print(round(rbind(x, MC1, MC2, Phi), 5))
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1.95
for (i in 1:m) {
MC1[i] <- MC.Phi(x, R = 1000, anti = FALSE)
MC2[i] <- MC.Phi(x, R = 1000)
}
print(sd(MC1))
print(sd(MC2))
print((var(MC1) - var(MC2))/var(MC1))
m <- 10000
a <- - 12 + 6 * (exp(1) - 1)
U <- runif(m)
T1 <- exp(U)                  #simple MC
T2 <- exp(U) + a * (U - 1/2)  #controlled
c(mean(T1), sd(T1))
c(mean(T2), sd(T2))
(var(T1) - var(T2)) / var(T1)
f <- function(u)
exp(-.5)/(1+u^2)
g <- function(u)
exp(-u)/(1+u^2)
set.seed(510) #needed later
u <- runif(10000)
B <- f(u)
A <- g(u)
cor(A, B)
a <- -cov(A,B) / var(B)    #est of c*
a
m <- 100000
u <- runif(m)
T1 <- g(u)
T2 <- T1 + a * (f(u) - exp(-.5)*pi/4)
c(mean(T1), mean(T2))
c(var(T1), var(T2))
(var(T1) - var(T2)) / var(T1)
set.seed(510)
u <- runif(10000)
f <- exp(-.5)/(1+u^2)
g <- exp(-u)/(1+u^2)
L <- lm(g ~ f)
summary(L)
c.star <-  - L$coeff[2]   # beta_1
mu <- exp(-.5)*pi/4
c.star
a
theta.hat <- sum(L$coeff * c(1, mu))  # pred. value at mu
predict(L, newdata = data.frame(f=mu)) # alternative
theta.hat
summary(L)$sigma
summary(L)$r.squared
